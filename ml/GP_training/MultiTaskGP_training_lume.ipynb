{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c078802c-94d6-44e1-88d3-34f4b5cc8be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gpytorch==1.14\n",
    "# !pip install botorch==0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc60e202-99b7-4b64-8957-31d264d08acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import pymongo\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import yaml\n",
    "import json\n",
    "import botorch\n",
    "from botorch.models.transforms.input import AffineInputTransform\n",
    "from botorch.models import MultiTaskGP\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from botorch.models.transforms.input import Normalize\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcf542e-45a1-4cf5-be1d-938322f4d753",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BoTorch version:\", botorch.__version__)\n",
    "print(\"GPyTorch version:\", gpytorch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2125e381-4dd0-419c-8d69-ac2820011d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select experimental setup for which we are training a model\n",
    "setup = \"ip2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f114c8e-669a-4601-b3d1-5cdfb97ce265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open credential file for database\n",
    "with open(os.path.join(os.getenv('HOME'), 'db.profile')) as f:\n",
    "    db_profile = f.read()\n",
    "\n",
    "# Connect to the MongoDB database with read-only access\n",
    "db = pymongo.MongoClient(\n",
    "    host=\"mongodb05.nersc.gov\",\n",
    "    username=\"bella_sf_ro\",\n",
    "    password=re.findall('SF_DB_READONLY_PASSWORD=(.+)', db_profile)[0],\n",
    "    authSource=\"bella_sf\")[\"bella_sf\"]\n",
    "\n",
    "# Extract data from the database as pandas dataframe\n",
    "collection = db[setup]\n",
    "df = pd.DataFrame( list(collection.find()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed03b2f-0f49-4879-9d5b-48f32f10351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the name of inputs and outputs for this setup\n",
    "with open(\"../../dashboard/config/variables.yml\") as f:\n",
    "    yaml_dict = yaml.safe_load( f.read() )\n",
    "input_variables = yaml_dict[setup][\"input_variables\"]\n",
    "input_names = [ v['name'] for v in input_variables.values() ] \n",
    "output_variables = yaml_dict[setup][\"output_variables\"]\n",
    "output_names = [ v['name'] for v in output_variables.values() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8c4d2a-2956-4d09-8b22-72edd2c58934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dimensional data\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "\n",
    "ax.scatter( \n",
    "    df[input_names[0]], \n",
    "    df[input_names[-1]], \n",
    "    df[output_names[0]], \n",
    "    c=df.experiment_flag, \n",
    "    alpha=0.3)\n",
    "\n",
    "ax.view_init(elev=40., azim=40, roll=0)\n",
    "plt.xlabel(input_names[0])\n",
    "plt.ylabel(input_names[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015ecbe3-a682-452f-b363-88c993dd1cb5",
   "metadata": {},
   "source": [
    "# Normalize with affine input/output transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f71c852-73fe-447a-9768-d34c5b63e940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input and output normalizations\n",
    "X = torch.tensor( df[ input_names ].values, dtype=torch.float )\n",
    "\n",
    "# Forward input and output transforms\n",
    "input_transform = AffineInputTransform( \n",
    "    len(input_names), \n",
    "    coefficient=X.std(axis=0), \n",
    "    offset=X.mean(axis=0)\n",
    ")\n",
    "\n",
    "def reverse_affine_transform(x_normalized, transform):\n",
    "    return transform.coefficient * x_normalized + transform.offset\n",
    "    \n",
    "y = torch.tensor( df[ output_names ].values, dtype=torch.float )\n",
    "output_transform = AffineInputTransform( \n",
    "    len(output_names), \n",
    "    coefficient=y.std(axis=0),\n",
    "    offset=y.mean(axis=0)\n",
    ")\n",
    "\n",
    "# Reverse output transformations\n",
    "output_transform_reversed = AffineInputTransform(\n",
    "    d=len(output_names),\n",
    "    coefficient=output_transform.coefficient,\n",
    "    offset=output_transform.offset,\n",
    "    reverse=True  \n",
    ")\n",
    "\n",
    "if (min(X.mean(axis=0)) == 0):\n",
    "    print(\"Mean value used for normalization is 0. This will lead to NaNs \",X.mean(axis=0))\n",
    "if (min(X.std(axis=0)) == 0):\n",
    "    print(\"RMS value used for normalization is 0. This will lead to NaNs \", X.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f2e165-35d4-4b8a-a0f6-1fc1c808bc0d",
   "metadata": {},
   "source": [
    "# Save input/output normalization parameters to JSON for future inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aafcda-4996-4811-b510-15813fbf2bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "normalization_info = {\n",
    "    \"input_mean\": input_transform.offset.tolist(),\n",
    "    \"input_std\": input_transform.coefficient.tolist(),\n",
    "    \"output_mean\": output_transform.offset.tolist(),\n",
    "    \"output_std\": output_transform.coefficient.tolist()\n",
    "}\n",
    "\n",
    "with open('./normalization/normalization.json', 'w') as f:\n",
    "    json.dump(normalization_info, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b04287-90e3-438c-ab42-5bc52917ce7f",
   "metadata": {},
   "source": [
    "# Apply normalization to the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc2e33b-3bba-45fc-86c1-a6918d13f5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df = df.copy()\n",
    "norm_df[input_names] = input_transform( torch.tensor( df[input_names].values ) )\n",
    "norm_df[output_names] = output_transform( torch.tensor( df[output_names].values ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb8c1b9-1af9-458c-870e-81bb18478ccd",
   "metadata": {},
   "source": [
    "# Visualize the normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0221842e-b3cf-453d-a2ac-195fe118b267",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "\n",
    "ax.scatter( \n",
    "    norm_df[input_names[0]], \n",
    "    norm_df[input_names[-1]], \n",
    "    norm_df[output_names[0]], \n",
    "    c=norm_df.experiment_flag, \n",
    "    alpha=0.3)\n",
    "\n",
    "ax.view_init(elev=40., azim=40, roll=0)\n",
    "plt.xlabel(input_names[0])\n",
    "plt.ylabel(input_names[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663d199d-eaec-4624-9658-90f93e43eab7",
   "metadata": {},
   "source": [
    "# Define a multi-input multi-task GP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8cef86-310e-4f83-a3f3-c9f70871aec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gp_model = MultiTaskGP(\n",
    "    torch.tensor( norm_df[['experiment_flag']+input_names].values ),\n",
    "    torch.tensor( norm_df[output_names].values ),\n",
    "    task_feature=0,\n",
    "    covar_module=ScaleKernel(RBFKernel()),\n",
    "    outcome_transform=None,\n",
    ")\n",
    "# Fit the model\n",
    "mll = ExactMarginalLogLikelihood(gp_model.likelihood, gp_model)\n",
    "fit_gpytorch_mll(mll)\n",
    "\n",
    "# cov = gp_model.task_covar_module._eval_covar_matrix()\n",
    "# print( 'Correlation: ', cov[1,0]/torch.sqrt(cov[0,0]*cov[1,1]).item() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a950251b-8554-43e6-ac10-dfa6ff14990b",
   "metadata": {},
   "source": [
    "# Construct LUME GPModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3a8dd8-307d-496a-a549-b4abd1d3dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall --yes lume-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d0f352-45e1-4626-a85e-9c0744d15228",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/slaclab/lume-model.git@41fd8a5a3144a7bddca39b806500caabe2187262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff0d305-ba13-40ed-8d73-5f1e23f4d440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lume_model.variables import ScalarVariable, DistributionVariable\n",
    "from lume_model.models.gp_model import GPModel\n",
    "\n",
    "input_variables = [\n",
    "    ScalarVariable(name=\"TOD_fs3\"),\n",
    "    ScalarVariable(name=\"GVD\"),\n",
    "    ScalarVariable(name=\"z_target_um\"),   \n",
    "]\n",
    "output_variables = [\n",
    "    DistributionVariable(name=\"n_protons_sim_task\", distribution_type=\"MultiVariateNormal\"),\n",
    "    DistributionVariable(name=\"n_protons_exp_task\", distribution_type=\"MultiVariateNormal\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d5b7db-3baf-46f9-8e8f-23ca11340a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_model.eval()\n",
    "\n",
    "gp_lume_model = GPModel(\n",
    "    model=gp_model, input_variables=input_variables, output_variables=output_variables, input_transformers=[], output_transformers=[], jitter=1e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5240660f-e820-4831-9409-2c09956f4a71",
   "metadata": {},
   "source": [
    "# Print and plot normalized predictions by running lume GP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6967789f-48a6-4558-8564-61b41037a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions on physical (unnormalized) data\n",
    "tod_max = norm_df['TOD_fs3'].max()\n",
    "gvd_fix = (norm_df['GVD'].max()+ norm_df['GVD'].min())/2\n",
    "zmin, zmax = norm_df['z_target_um'].min(), norm_df['z_target_um'].max()\n",
    "\n",
    "z_test_array = torch.tensor (np.linspace(zmin,zmax,100).reshape(-1, 1), dtype=torch.float32) \n",
    "TOD_test_array  = torch.tensor (np.linspace(tod_max,tod_max,100).reshape(-1, 1), dtype=torch.float32) \n",
    "GVD_test_array  = torch.tensor (np.linspace(gvd_fix,gvd_fix,100).reshape(-1, 1), dtype=torch.float32)  #0.1\n",
    "\n",
    "input_dict = {\n",
    "    'TOD_fs3': TOD_test_array.squeeze(1).to(dtype=torch.double),\n",
    "    'GVD': GVD_test_array.squeeze(1).to(dtype=torch.double),\n",
    "    'z_target_um': z_test_array.squeeze(1).to(dtype=torch.double),\n",
    "}\n",
    "output_dict = gp_lume_model.evaluate(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287a9236-d23e-4d98-bf94-c91b722095fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "l1, u1 = (\n",
    "    output_dict[\"n_protons_sim_task\"].mean.sub(output_dict[\"n_protons_sim_task\"].variance.sqrt().mul_(2)),\n",
    "    output_dict[\"n_protons_sim_task\"].mean.add(output_dict[\"n_protons_sim_task\"].variance.sqrt().mul_(2)),\n",
    ")\n",
    "l2, u2 = (\n",
    "    output_dict[\"n_protons_exp_task\"].mean.sub(output_dict[\"n_protons_exp_task\"].variance.sqrt().mul_(2)),\n",
    "    output_dict[\"n_protons_exp_task\"].mean.add(output_dict[\"n_protons_exp_task\"].variance.sqrt().mul_(2)),\n",
    ")\n",
    "\n",
    "plt.fill_between(\n",
    "    z_test_array.squeeze().numpy(), l2.detach().numpy(), u2.detach().numpy(), alpha=0.25, lw=0, color=\"C0\"\n",
    ")\n",
    "plt.fill_between(\n",
    "    z_test_array.squeeze().numpy(), l1.detach().numpy(), u1.detach().numpy(), alpha=0.25, lw=0, color=\"C1\"\n",
    ")\n",
    "\n",
    "plt.ylabel(\"n_protons\")\n",
    "\n",
    "plt.scatter(\n",
    "    z_test_array.squeeze().numpy(),\n",
    "    output_dict['n_protons_sim_task'].mean.detach().numpy(),\n",
    "    color=\"C1\",\n",
    "    lw=1,\n",
    "    label=\"Multi-fidelity GP prediction\\n for low-fidelity (sim) data\",\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    z_test_array.squeeze().numpy(),\n",
    "    output_dict['n_protons_exp_task'].mean.detach().numpy(),\n",
    "    color=\"C0\",\n",
    "    lw=1,\n",
    "    label=\"Multi-fidelity GP prediction\\n for high-fidelity (exp) data\",\n",
    ")\n",
    "\n",
    "plt.legend(loc=0, fontsize=\"small\")\n",
    "plt.grid()\n",
    "plt.xlabel('z_target_um')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdb4beb-17ca-4667-ab04-72aa76a07cec",
   "metadata": {},
   "source": [
    "# Save lume GP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe97cd7-6421-452b-a146-87361af4f225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "gp_lume_model.dump(\n",
    "    file='./saved_models/' + setup +'.yml',\n",
    "    save_models=True,    # this saves the underlying botorch/gpytorch model to a .pth file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fcdb22-eaa4-451a-9912-5de7759eb7ef",
   "metadata": {},
   "source": [
    "# Created a PhysicalGPModel class to get predictions and uncertainty in physical units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e432c2-5a62-44e6-81ce-12531fe29df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a PhysicalGPModel Class: \n",
    "#    - looading normalization;\n",
    "#    - predicting with physical units & uncertainty\n",
    "\n",
    "class PhysicalGPModel:\n",
    "    def __init__(self, lume_gp, norm_json_path):\n",
    "        with open(norm_json_path, \"r\") as f:\n",
    "            norm = json.load(f)\n",
    "\n",
    "        self.lume_gp = lume_gp  # Store the full LUME GP model\n",
    "        self.input_mean = torch.tensor(norm['input_mean'], dtype=torch.float32)\n",
    "        self.input_std = torch.tensor(norm['input_std'], dtype=torch.float32)\n",
    "        self.output_mean = torch.tensor(norm['output_mean'], dtype=torch.float32)\n",
    "        self.output_std = torch.tensor(norm['output_std'], dtype=torch.float32)\n",
    "\n",
    "    def predict(self, physical_input_dict):\n",
    "        # Stack input values into a tensor in correct order\n",
    "        input_tensor = torch.stack([\n",
    "            physical_input_dict[name] for name in self.lume_gp.input_names\n",
    "        ], dim=-1)\n",
    "\n",
    "        # Normalize inputs\n",
    "        norm_input = (input_tensor - self.input_mean) / self.input_std\n",
    "\n",
    "        # Get posterior from BoTorch model\n",
    "        posterior = self.lume_gp.model.posterior(norm_input)\n",
    "\n",
    "        # Convert normalized outputs back to physical units\n",
    "        transformed_mean = posterior.mean * self.output_std + self.output_mean\n",
    "        transformed_variance = posterior.variance * (self.output_std ** 2)\n",
    "\n",
    "        # Build result dictionary with the same structure\n",
    "        output_dict = {\n",
    "            name: torch.distributions.MultivariateNormal(\n",
    "                loc=transformed_mean[..., i],\n",
    "                covariance_matrix=torch.diag(transformed_variance[..., i])\n",
    "            )\n",
    "            for i, name in enumerate(self.lume_gp.output_names)\n",
    "        }\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3594ee-c0aa-4d77-a4ff-b59baa9dad16",
   "metadata": {},
   "source": [
    "# Load lume GP model and print corresponding predictions and uncertainty in physical units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34775337-430e-4a59-aa6c-607aea748220",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_lume_gp_model = GPModel.from_yaml(\"./saved_models/ip2.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f714bf5-b78f-47fa-947e-2004a05df7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tod_max = df['TOD_fs3'].max()\n",
    "zmin, zmax = df['z_target_um'].min(), df['z_target_um'].max()\n",
    "gvd_fix = (df['GVD'].max()+ df['GVD'].min())/2\n",
    "\n",
    "z_test_array = torch.tensor (np.linspace(zmin,zmax,100).reshape(-1, 1), dtype=torch.float32) \n",
    "TOD_test_array  = torch.tensor (np.linspace(tod_max,tod_max,100).reshape(-1, 1), dtype=torch.float32) \n",
    "GVD_test_array  = torch.tensor (np.linspace(gvd_fix,gvd_fix,100).reshape(-1, 1), dtype=torch.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf41728-5f21-4ef9-a10e-c6c50ddb53fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict_phys = {\n",
    "    'TOD_fs3': TOD_test_array.squeeze(1).to(dtype=torch.float32),\n",
    "    'GVD': GVD_test_array.squeeze(1).to(dtype=torch.double),\n",
    "    'z_target_um': z_test_array.squeeze(1).to(dtype=torch.float32),\n",
    "}\n",
    "model = PhysicalGPModel(loaded_lume_gp_model, \"./normalization/normalization.json\")\n",
    "output_dict_phys = model.predict(input_dict_phys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0893508c-3843-49f5-a8dd-5958386a7128",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "l1, u1 = (\n",
    "    output_dict_phys['n_protons_sim_task'].mean - 2. * output_dict_phys['n_protons_sim_task'].variance.sqrt(),\n",
    "    output_dict_phys['n_protons_sim_task'].mean + 2. * output_dict_phys['n_protons_sim_task'].variance.sqrt(),\n",
    ")\n",
    "l2, u2 = (\n",
    "    output_dict_phys['n_protons_exp_task'].mean - 2. * output_dict_phys['n_protons_exp_task'].variance.sqrt(),\n",
    "    output_dict_phys['n_protons_exp_task'].mean + 2. * output_dict_phys['n_protons_exp_task'].variance.sqrt(),\n",
    ")\n",
    "\n",
    "plt.fill_between(\n",
    "    z_test_array.squeeze().numpy(), l2.detach().numpy(), u2.detach().numpy(), alpha=0.25, lw=0, color=\"C0\"\n",
    ")\n",
    "plt.fill_between(\n",
    "    z_test_array.squeeze().numpy(), l1.detach().numpy(), u1.detach().numpy(), alpha=0.25, lw=0, color=\"C1\"\n",
    ")\n",
    "\n",
    "plt.ylabel(\"n_protons\")\n",
    "\n",
    "plt.scatter(\n",
    "    z_test_array.squeeze().numpy(),\n",
    "    output_dict_phys[\"n_protons_sim_task\"].mean.detach().numpy(),\n",
    "    color=\"C1\",\n",
    "    lw=1,\n",
    "    label=\"Multi-fidelity GP prediction\\n for low-fidelity (sim) data\",\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    z_test_array.squeeze().numpy(),\n",
    "    output_dict_phys[\"n_protons_exp_task\"].mean.detach().numpy(),\n",
    "    color=\"C0\",\n",
    "    lw=1,\n",
    "    label=\"Multi-fidelity GP prediction\\n for high-fidelity (exp) data\",\n",
    ")\n",
    "\n",
    "plt.legend(loc=0, fontsize=\"small\")\n",
    "plt.grid()\n",
    "plt.xlabel('z_target_um')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.1.0",
   "language": "python",
   "name": "pytorch-2.1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
