{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook shows how to define a kernel that is robust to uncalibrated data in the input dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from botorch.models import MultiTaskGP\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robustness to wrong calibration in output\n",
    "\n",
    "The multifidelity kernel is naturally robust to mismatch between the two fidelities in the output dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from typing import Union\n",
    "from gpytorch.distributions.multivariate_normal import MultivariateNormal\n",
    "from gpytorch.module import Module\n",
    "from botorch.models.transforms.input import InputTransform\n",
    "from botorch.models.transforms.outcome import OutcomeTransform\n",
    "from botorch.utils.types import _DefaultType, DEFAULT\n",
    "from gpytorch.likelihoods.likelihood import Likelihood\n",
    "from gpytorch.priors.prior import Prior\n",
    "\n",
    "class ShiftedMultiTaskGP(MultiTaskGP):\n",
    "    \"\"\"Multi-task GP with task-specific input shifts.\n",
    "\n",
    "    This model extends MultiTaskGP by adding task-specific shift parameters\n",
    "    Δx_{s,s'} to the kernel, resulting in a kernel of the form:\n",
    "    k[(s,x), (s',x')] = κ_{s,s'} * k(x - x' - Δx_{s,s'})\n",
    "\n",
    "    where κ_{s,s'} is the task covariance and k is the base kernel.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_X: Tensor,\n",
    "        train_Y: Tensor,\n",
    "        task_feature: int,\n",
    "        train_Yvar: Union[Tensor, None] = None,\n",
    "        mean_module: Union[Module, None] = None,\n",
    "        covar_module: Union[Module, None] = None,\n",
    "        likelihood: Union[Likelihood , None]= None,\n",
    "        task_covar_prior: Union[Prior , None] = None,\n",
    "        output_tasks: Union[list[int] , None] = None,\n",
    "        rank: Union[int , None] = None,\n",
    "        all_tasks: Union[list[int] , None] = None,\n",
    "        outcome_transform: Union[OutcomeTransform, _DefaultType, None] = None,\n",
    "        input_transform: Union[InputTransform , None] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            train_X=train_X,\n",
    "            train_Y=train_Y,\n",
    "            task_feature=task_feature,\n",
    "            train_Yvar=train_Yvar,\n",
    "            mean_module=mean_module,\n",
    "            covar_module=covar_module,\n",
    "            likelihood=likelihood,\n",
    "            task_covar_prior=task_covar_prior,\n",
    "            output_tasks=output_tasks,\n",
    "            rank=rank,\n",
    "            all_tasks=all_tasks,\n",
    "            outcome_transform=outcome_transform,\n",
    "            input_transform=input_transform,\n",
    "        )\n",
    "        # Initialize task-specific shifts\n",
    "        self.input_dim = train_X.shape[-1] - 1  # Subtract task feature\n",
    "        self.register_parameter(name=\"shifts\", parameter=torch.nn.Parameter(torch.zeros(self.num_tasks, self.input_dim)))\n",
    "        \n",
    "\n",
    "    def forward(self, x: Tensor) -> MultivariateNormal:\n",
    "        if self.training:\n",
    "            x = self.transform_inputs(x)\n",
    "        x_basic, task_idcs = self._split_inputs(x)\n",
    "        # Apply shifts\n",
    "        shifts = self.shifts[task_idcs.reshape(-1)]\n",
    "        x_basic = x_basic - shifts\n",
    "        # Compute base mean and covariance\n",
    "        mean_x = self.mean_module(x_basic)\n",
    "        covar_x = self.covar_module(x_basic)\n",
    "        # Compute task covariances\n",
    "        covar_i = self.task_covar_module(task_idcs)\n",
    "        # Combine the two in an ICM fashion\n",
    "        covar = covar_x.mul(covar_i)\n",
    "        return MultivariateNormal(mean_x, covar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gp(gp, X, y, plot=False):\n",
    "    x0 = torch.linspace(l1,u1, 100).reshape(-1,1)\n",
    "    y0 = torch.linspace(l2,u2, 100).reshape(-1,1)\n",
    "    \n",
    "    x = torch.cat([x0,y0], dim=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        p = gp.posterior(torch.cat([x0,y0], dim=1))\n",
    "        m = p.mean\n",
    "        l,u = p.mvn.confidence_region()\n",
    "    # Extract correlation coefficient\n",
    "    cov = gp.task_covar_module._eval_covar_matrix()\n",
    "    corr = cov[1,0]/torch.sqrt(cov[0,0]*cov[1,1]).item()\n",
    "    \n",
    "    if plot==True:\n",
    "        plt.figure()\n",
    "    \n",
    "        plt.ylabel('$f$')\n",
    "    \n",
    "        plt.fill_between(x0.squeeze(), l[:,1].detach().numpy(), u[:,1].detach().numpy(), alpha = 0.25, lw = 0, color='C0')\n",
    "        plt.fill_between(x0.squeeze(), l[:,0].detach().numpy(), u[:,0].detach().numpy(), alpha = 0.25, lw = 1, color='C1')\n",
    "    \n",
    "        plt.scatter(X[:,0][X[:,-1]==0], y[X[:,-1]==0],  color='C1', label='Low-fidelity data')\n",
    "        plt.scatter(X[:,0][X[:,-1]==1],  y[X[:,-1]==1], color='C0', label='High-fidelity data')\n",
    "    \n",
    "        plt.plot(x0, m[:,-1], color='C0', lw=1, label='Multi-fidelity GP prediction\\n for high-fidelity data')\n",
    "        plt.plot(x0, m[:,0], color='C1', lw=1, label='Multi-fidelity GP prediction\\n for low-fidelity data')\n",
    "\n",
    "        plt.legend(loc=0, fontsize='small')\n",
    "    \n",
    "        plt.title( f'Correlation coefficient: {corr:.2f}' )\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f'Correlation coefficient: {corr:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and visualize synthetic data with 2 input features and 1 output feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_exp(x,y):\n",
    "    return 5.*np.sin(x+y) + np.exp(-0.1*x**2 - 0.1*y**2) \n",
    "\n",
    "shift0 = 1.5\n",
    "shift1 = -0.35 \n",
    "\n",
    "x1 = 3 * torch.rand(20, dtype=torch.float64)  \n",
    "y1 = 4 * torch.rand(20, dtype=torch.float64)  \n",
    "\n",
    "x2 = 3 * torch.rand(15, dtype=torch.float64)  \n",
    "y2 = 4 * torch.rand(15, dtype=torch.float64)  \n",
    "    \n",
    "z1 =  fun_exp(x1,y1).reshape(-1, 1)\n",
    "z2 =  fun_exp(x2-shift0,y2-shift1).reshape(-1, 1)  \n",
    "    \n",
    "l1 = 0\n",
    "u1 = 3\n",
    "l2 = 0\n",
    "u2 = 4\n",
    "\n",
    "X1_stack = torch.stack([x1,y1, torch.zeros_like(x1)],dim=-1)\n",
    "X2_stack = torch.stack([x2,y2, torch.ones_like(x2)],dim=-1)\n",
    "\n",
    "X = torch.cat([X1_stack, X2_stack])\n",
    "y = torch.cat([z1,z2]).reshape(-1, 1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x1,y1,z1, label = 'Low-res data')\n",
    "ax.scatter(x2,y2,z2, label = 'High-res data (with a shift)')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print correlation coefficient by running MultiTaskGP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and plot GP (do 3 different training to check for robustness)\n",
    "print('MultiTaskGP model:\\n')\n",
    "for _ in range(3):\n",
    "    gp = MultiTaskGP(X, y, task_feature=-1, rank=2)\n",
    "    mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "    fit_gpytorch_mll(mll);\n",
    "    plot_gp(gp, X, y, plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print correlation coefficient by running ShiftedMultiTaskGP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and plot GP (do 3 different training to check for robustness)\n",
    "print('ShiftedMultiTaskGP model:\\n')\n",
    "for _ in range(3):\n",
    "    gp = ShiftedMultiTaskGP(X, y, task_feature=-1, rank=2)\n",
    "    mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "    fit_gpytorch_mll(mll)\n",
    "    plot_gp(gp, X, y, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
