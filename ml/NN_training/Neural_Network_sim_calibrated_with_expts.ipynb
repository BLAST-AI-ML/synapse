{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1504bb8-c95a-4f64-bb7a-895025599837",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook shows how to train and test a neural network on the BELLA data.\n",
    "\n",
    "In order to run this notebook, you need to produce first produce CSV files for the training and testing data. (See the folder `experimental data`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d5a42e-94a9-4438-b4a5-0f14013e958a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import transformer\n",
    "\n",
    "from Neural_Net_Classes import NN as NN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c9e20-78a3-4852-a0ba-67fee8a5aad2",
   "metadata": {},
   "source": [
    "<h2>Loading Split Experimental Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008ee17f-cef5-4794-a690-9352d8c741ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "sim_training_set_df = pd.read_csv('simulation_data.csv')\n",
    "\n",
    "# Access the arrays\n",
    "sim_z_training_set = sim_training_set_df['z_target (m)'].values\n",
    "sim_TOD_training_set = sim_training_set_df['TOD (s^3)'].values\n",
    "sim_protons_training_set = sim_training_set_df['n_protons (1/sr)'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ee04d6-379b-408b-88ec-11392ed80b78",
   "metadata": {},
   "source": [
    "<h2>Visualizing Split Experimental Datasets</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c68dcf5-befc-4481-9a48-3049a5aa3ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "\n",
    "ax.scatter( sim_TOD_training_set, sim_z_training_set, sim_protons_training_set, c='r',alpha=0.3, label='Training Set')\n",
    "#ax.scatter( TOD_test_set, z_test_set,protons_test_set, c='b', alpha=0.3, label='Testing Set')\n",
    "ax.view_init(elev=40., azim=40)\n",
    "plt.xlabel('TOD')\n",
    "plt.ylabel('z_target')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa2d876-bff4-4208-ae51-2da061a64489",
   "metadata": {},
   "source": [
    "<h2>Normalizing Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4af4e4-a288-4840-936a-99e466653a1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Setting Bounds\n",
    "z_bounds = torch.tensor([-150e-6, 150e-6])\n",
    "TOD_bounds = torch.tensor([-80e-41, 80e-41])\n",
    "protons_bounds = torch.tensor([7e7, 1e11])\n",
    "\n",
    "#define transformers\n",
    "sim_transformer_z = transformer.Transformer(z_bounds.reshape(2,1), transform_type = 'normalize')\n",
    "sim_transformer_TOD = transformer.Transformer(TOD_bounds.reshape(2,1), transform_type = 'normalize')\n",
    "sim_transformer_protons = transformer.Transformer(protons_bounds.reshape(2,1), transform_type = 'normalize')\n",
    "\n",
    "#Full normalization process\n",
    "def normalization(array, transformer):\n",
    "    array = np.array(array).reshape(-1,1)\n",
    "    array = torch.tensor(array)\n",
    "    norm = transformer.forward(array)\n",
    "    return norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0515b3a3-5c31-4035-af39-4f919e28aa0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Normalize datasets 1\n",
    "norm_sim_z_train_set = normalization(sim_z_training_set, sim_transformer_z)\n",
    "norm_sim_TOD_train_set = normalization(sim_TOD_training_set, sim_transformer_TOD)\n",
    "norm_sim_protons_train_set = normalization(sim_protons_training_set, sim_transformer_protons)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bbc154-8a05-4243-a721-8ca6605f66df",
   "metadata": {},
   "source": [
    "<h2>Visualizing Normalized Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbcd5db-2f85-47a8-895d-662ac9b080dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Scatter plot for training set\n",
    "ax.scatter(norm_sim_TOD_train_set, norm_sim_z_train_set, norm_sim_protons_train_set, label='Sim Training Set', alpha=0.7)\n",
    "# Scatter plot for testing set\n",
    "#ax.scatter(norm_TOD_test_set, norm_z_test_set, norm_protons_test_set, label='Test Set', alpha=0.7)\n",
    "ax.view_init(elev=40., azim=40)\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Normalized TOD')\n",
    "ax.set_ylabel('Normalized Z')\n",
    "ax.set_zlabel('Normalized Protons')\n",
    "\n",
    "# Add legend\n",
    "ax.legend()\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1033c486-c3ee-47a5-967d-61d3cb931179",
   "metadata": {},
   "source": [
    "<h1>Neural Network Framework</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12224782-1c17-4eb6-a537-0805349e25c1",
   "metadata": {},
   "source": [
    "<h2>Build and Train Neural Networks on simulation data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad31b07-cd0e-40a7-b529-1da5e72fdd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_net = NN(learning_rate=0.0001)\n",
    "sim_net.train_model(norm_sim_z_train_set, norm_sim_TOD_train_set, norm_sim_protons_train_set,num_epochs=10000)\n",
    "sim_net.plot_loss()\n",
    "#net.test_model(norm_z_test_set, norm_TOD_test_set, norm_protons_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b103604-d92b-4e9e-9a2f-d578feffd1d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_predictions = sim_net.predict(norm_sim_z_train_set, norm_sim_TOD_train_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc81392-9a19-4b27-a244-751b1c192d2a",
   "metadata": {},
   "source": [
    "<h2>Plotting Predictions from NN trained on simulations </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4bbd0-5dbd-4fb0-ac00-02b0cd29d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(norm_sim_z_train_set, norm_sim_protons_train_set, label='Training Set 1')\n",
    "\n",
    "\n",
    "ax.scatter(train_predictions['Z_target'], train_predictions['predictions'], label='predictions', s=50, facecolors='none', edgecolors='r')\n",
    "\n",
    "plt.title(\"n_protons predictions\")\n",
    "plt.xlabel('z_target (m)')\n",
    "plt.ylabel('Number of protons (1/sr)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9faf97-c958-4e86-a8e4-3ce356cbf4ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Scatter plot for training set\n",
    "ax.scatter(norm_sim_TOD_train_set, norm_sim_z_train_set, norm_sim_protons_train_set, label='Training Set 1', alpha=0.7)\n",
    "#ax.scatter(norm_TOD_test_set, norm_z_test_set, norm_protons_test_set, label='Test Set 1', alpha=0.7)\n",
    "\n",
    "ax.scatter(train_predictions['TOD'], train_predictions['Z_target'], train_predictions['predictions'], label='predictions 1', s=50, facecolors='none', edgecolors='r')\n",
    "#ax.scatter(test_predictions['TOD'], test_predictions['Z_target'], test_predictions['predictions'], s=50, facecolors='none', edgecolors='r')\n",
    "\n",
    "ax.view_init(elev=40., azim=40)\n",
    "# Set labels and title\n",
    "ax.set_title('Simulation Data v Predictions')\n",
    "ax.set_xlabel('TOD')\n",
    "ax.set_ylabel('z_target')\n",
    "ax.set_zlabel('n Protons')\n",
    "\n",
    "# Add legend\n",
    "ax.legend()\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1788e8-41e1-43a2-851d-b896338711a5",
   "metadata": {},
   "source": [
    "<h2> Define NN that adds linear calibration parameters to output obtained from pre-trained base neural net </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251c33df-eba1-4465-9f32-826b0559ce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class finetune_NN(NN):\n",
    "    def __init__(self, base_model, hidden_size=20, \n",
    "                 learning_rate=0.001, patience=100, factor=0.5, threshold=1e-4):\n",
    "        super(finetune_NN, self).__init__(hidden_size, learning_rate, patience, factor, threshold)\n",
    "        \n",
    "        # Copying weights from base_model to this model\n",
    "        self.hidden1.load_state_dict(base_model.hidden1.state_dict())\n",
    "        self.hidden2.load_state_dict(base_model.hidden2.state_dict())\n",
    "        self.hidden3.load_state_dict(base_model.hidden3.state_dict())\n",
    "        self.hidden4.load_state_dict(base_model.hidden4.state_dict())\n",
    "        self.hidden5.load_state_dict(base_model.hidden5.state_dict())        \n",
    "        self.output.load_state_dict(base_model.output.state_dict())\n",
    "        # This is the weight and bias to be tuned on the output obtained from base model\n",
    "        self.w2 =torch.nn.Parameter(torch.rand(1))\n",
    "        self.b2 = torch.nn.Parameter(torch.rand(1))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, 'min', \n",
    "                                           factor=factor, patience=patience, threshold=threshold)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.relu(self.hidden2(x))\n",
    "        x = self.relu(self.hidden3(x))\n",
    "        x = self.relu(self.hidden4(x))\n",
    "        x = self.relu(self.hidden5(x))\n",
    "        x = self.output(x)\n",
    "        x = x * self.w2 + self.b2\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def train_model(self, x_train, y_train, z_train, num_epochs=1500):\n",
    "        #print(self.parameters())\n",
    "        for param in self.hidden1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.hidden2.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.hidden3.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.hidden4.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.hidden5.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.output.parameters():\n",
    "            param.requires_grad = False           \n",
    "  \n",
    "        x_train = x_train.to(torch.float32)\n",
    "        y_train = y_train.to(torch.float32)\n",
    "        z_train = z_train.to(torch.float32)\n",
    "        \n",
    "        inputs = torch.cat((x_train, y_train), dim=1).to(torch.float32)\n",
    "        self.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            outputs = self(inputs)\n",
    "            loss = self.criterion(outputs, z_train)\n",
    "            loss.backward()\n",
    "            torch.autograd.set_detect_anomaly(True)            \n",
    "            self.optimizer.step()\n",
    "\n",
    "            current_loss = loss.item()\n",
    "            self.loss_data['loss'].append(current_loss)\n",
    "            self.loss_data['epoch_count'].append(epoch)\n",
    "            self.scheduler.step(current_loss)\n",
    "\n",
    "            if (epoch + 1) % (num_epochs/1000) == 0:\n",
    "                print(f'Comb NN: Epoch [{epoch+1}/{num_epochs}], Loss:{loss.item():.6f}')\n",
    "                print(\"w2,b2\",self.w2,self.b2)\n",
    "\n",
    "    def predict(self, x_values, y_values):\n",
    "        '''\n",
    "        args:\n",
    "            tensor x_values\n",
    "            tensor y_values\n",
    "        returns:\n",
    "            numpy array with predictions\n",
    "        '''\n",
    "        predictions = {\n",
    "        'Z_target': x_values.tolist(),\n",
    "        'TOD': y_values.tolist(),\n",
    "        'predictions': []\n",
    "        }\n",
    "    \n",
    "        inputs = torch.cat((x_values, y_values), dim=1).to(torch.float32)\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self(inputs)\n",
    "            predictions['predictions'] = output.detach().numpy().tolist()\n",
    "    \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ffb1c7-1345-4aad-84e8-da4d4a2dfea0",
   "metadata": {},
   "source": [
    "<h1> Load experimental data previously split into training and testing </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d5d6a2-37c7-48f6-984f-bed2cf03fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "expt_training_set_df = pd.read_csv('expt_training_set_1.csv')\n",
    "\n",
    "# Access the arrays\n",
    "expt_z_training_set = expt_training_set_df['z_target (m)'].values\n",
    "expt_TOD_training_set = expt_training_set_df['TOD (s^3)'].values\n",
    "expt_protons_training_set = expt_training_set_df['n_protons (1/sr)'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360326f5-5a56-4e51-9b5a-15d6a2b6d9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "expt_test_set_df = pd.read_csv('expt_test_set_1.csv')\n",
    "\n",
    "# Access the arrays\n",
    "expt_z_test_set = expt_test_set_df['z_target (m)'].values\n",
    "expt_TOD_test_set = expt_test_set_df['TOD (s^3)'].values\n",
    "expt_protons_test_set = expt_test_set_df['n_protons (1/sr)'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a6fe9-4313-46db-adc4-8e0aae156188",
   "metadata": {},
   "source": [
    "<h2> Set bounds and normalize expt data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d8fb7a-6cef-4c88-a670-032f8756cf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting Bounds\n",
    "\n",
    "\n",
    "#define transformers\n",
    "expt_transformer_z = transformer.Transformer(z_bounds.reshape(2,1), transform_type = 'normalize')\n",
    "expt_transformer_TOD = transformer.Transformer(TOD_bounds.reshape(2,1), transform_type = 'normalize')\n",
    "expt_transformer_protons = transformer.Transformer(protons_bounds.reshape(2,1), transform_type = 'normalize')\n",
    "\n",
    "#Full normalization process\n",
    "def normalization(array, transformer):\n",
    "    array = np.array(array).reshape(-1,1)\n",
    "    array = torch.tensor(array)\n",
    "    norm = transformer.forward(array)\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09982b5-0fdc-4b2d-9b52-7bbebe89cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize datasets 1\n",
    "expt_norm_z_train_set = normalization(expt_z_training_set, expt_transformer_z)\n",
    "expt_norm_TOD_train_set = normalization(expt_TOD_training_set, expt_transformer_TOD)\n",
    "expt_norm_protons_train_set = normalization(expt_protons_training_set, expt_transformer_protons)\n",
    "expt_norm_z_test_set = normalization(expt_z_test_set, expt_transformer_z)\n",
    "expt_norm_TOD_test_set = normalization(expt_TOD_test_set, expt_transformer_TOD)\n",
    "expt_norm_protons_test_set = normalization(expt_protons_test_set, expt_transformer_protons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872c5895-67ae-47dd-b5a0-757a6c0d81fd",
   "metadata": {},
   "source": [
    "<h2> Train the linear scaling parameters on the output, namely, w2, b2, on the training set of experimental data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03234db3-d5c4-423d-9c10-0c932d2b61cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "finetune_net = finetune_NN(sim_net,learning_rate=0.001)\n",
    "finetune_net.train_model(expt_norm_z_train_set,expt_norm_TOD_train_set,expt_norm_protons_train_set,num_epochs=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04917891-6550-4b0e-a009-ad61e0276e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the loss and print the values of the learned calibration parameters \n",
    "finetune_net.plot_loss()\n",
    "for name, param in finetune_net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e9a2d2-7737-40ff-b514-7ef45e0aba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions from calibrated NN on testing data from experiments\n",
    "finetune_net_pred_expt_test = finetune_net.predict(expt_norm_z_test_set,expt_norm_TOD_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90282633-58d1-4a43-a8a6-f859ff54458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions from calibrated NN on testing data from experiments\n",
    "finetune_net_pred = finetune_net.predict(expt_norm_z_train_set,expt_norm_TOD_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32974ac-abb1-4eca-aa6e-fc46161858e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions from the base NN model (uncalibrated) on testing and training data from experiments\n",
    "sim_net_pred_expt_train = sim_net.predict(expt_norm_z_train_set,expt_norm_TOD_train_set)\n",
    "sim_net_pred_expt_test = sim_net.predict(expt_norm_z_test_set,expt_norm_TOD_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc156a5-c422-4db6-ac82-765a90e6d838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the data\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "ax.scatter(expt_norm_z_train_set, expt_norm_protons_train_set, label='Training Set 1')\n",
    "ax.scatter(expt_norm_z_test_set, expt_norm_protons_test_set, label='Test Set 1')\n",
    "ax.scatter(norm_sim_z_train_set, norm_sim_protons_train_set, label='Sim Training Set 1')\n",
    "\n",
    "ax.scatter(finetune_net_pred['Z_target'], finetune_net_pred['predictions'], label=' expt train predictions', s=50, facecolors='none', edgecolors='m')\n",
    "ax.scatter(finetune_net_pred_expt_test['Z_target'], finetune_net_pred_expt_test['predictions'],label=' expt test predictions', s=50, facecolors='none', edgecolors='r')\n",
    "ax.scatter(sim_net_pred_expt_train['Z_target'], sim_net_pred_expt_train['predictions'], label=' sim NN predictions for expt train', s=50, facecolors='none', edgecolors='cyan')\n",
    "ax.scatter(sim_net_pred_expt_test['Z_target'], sim_net_pred_expt_test['predictions'], label=' sim NN predictions for expt train', s=50, facecolors='none', edgecolors='blue')\n",
    "plt.title(\"n_protons predictions\")\n",
    "plt.xlabel('z_target (m)')\n",
    "plt.ylabel('Number of protons (1/sr)')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
