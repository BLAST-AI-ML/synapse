{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479b505f-f2f4-409c-a7b8-cbd69ca566c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from botorch.models.transforms.input import AffineInputTransform\n",
    "\n",
    "from Neural_Net_Classes import NN as NN\n",
    "\n",
    "import numpy as np\n",
    "#import transformer\n",
    "\n",
    "from Neural_Net_Classes import NN as NN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6371e389-3b34-42db-9536-2652fc815997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the simulaiton CSV file\n",
    "sim_training_set_df = pd.read_csv('../../simulation_data/simulation_data.csv')\n",
    "\n",
    "# Load the experimental train and test sets\n",
    "expt_training_set_df = pd.read_csv('../../experimental_data/training_set_1.csv')\n",
    "expt_test_set_df = pd.read_csv('../../experimental_data/test_set_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f73669-f49e-4a25-a93b-66fedb903465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the arrays\n",
    "sim_z_training_set = sim_training_set_df['z_target_um'].values\n",
    "sim_TOD_training_set = sim_training_set_df['TOD_fs3'].values\n",
    "sim_protons_training_set = sim_training_set_df['n_protons'].values\n",
    "\n",
    "plt.clf()\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "\n",
    "ax.scatter( sim_TOD_training_set, sim_z_training_set, sim_protons_training_set, c='r',alpha=0.3, label='Simulation training Set')\n",
    "ax.view_init(elev=40., azim=40)\n",
    "plt.xlabel('TOD')\n",
    "plt.ylabel('z_target')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d9ddf0-e765-414e-8a77-6814be225721",
   "metadata": {},
   "source": [
    "<h2> Normalize with Affine Input Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc27a4b-f2cb-494a-81a2-523352d9612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the input and output normalizations, based on the training set from experiments\n",
    "X = torch.tensor(expt_training_set_df[['z_target_um', 'TOD_fs3','GVD']].values, dtype=torch.float)\n",
    "input_transform = AffineInputTransform(3, coefficient=X.std(axis=0), offset=X.mean(axis=0))\n",
    "y = torch.tensor(expt_training_set_df['n_protons'].values, dtype=torch.float).reshape(-1,1)\n",
    "output_transform = AffineInputTransform( 1, coefficient=y.std(axis=0), offset=y.mean(axis=0))\n",
    "if (min(X.mean(axis=0)) == 0):\n",
    "    print(\"Mean value used for normalization is 0. This will lead to NaNs \",X.mean(axis=0))\n",
    "if (min(X.std(axis=0)) == 0):\n",
    "    print(\"RMS value used for normalization is 0. This will lead to NaNs \", X.std(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17226dec-08d5-462c-b722-1a118b5eac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply normalization to the training and test sets\n",
    "norm_sim_training_set_df = sim_training_set_df.copy()\n",
    "norm_sim_training_set_df[['z_target_um', 'TOD_fs3','GVD']] = input_transform( torch.tensor( sim_training_set_df[['z_target_um', 'TOD_fs3','GVD']].values ) )\n",
    "norm_sim_training_set_df[['n_protons']] = output_transform( torch.tensor( sim_training_set_df[['n_protons']].values.reshape(-1,1) ) )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f83d8-4488-485b-b4ee-acfd3231fd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter( norm_sim_training_set_df['TOD_fs3'], norm_sim_training_set_df['z_target_um'],\n",
    "           norm_sim_training_set_df['n_protons'], c='r', alpha=0.7, label='Training Set')\n",
    "\n",
    "ax.view_init(elev=40., azim=40, roll=0)\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Normalized TOD')\n",
    "ax.set_ylabel('Normalized Z')\n",
    "ax.set_zlabel('Normalized Protons')\n",
    "\n",
    "# Add legend\n",
    "ax.legend()\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a6445c-910b-4c6e-9c33-0422fed82429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.scatter(norm_sim_training_set_df['z_target_um'],\n",
    "           norm_sim_training_set_df['n_protons'], c='r', alpha=0.7, label='Training Set')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Normalized z')\n",
    "ax.set_ylabel('Normalized protons')\n",
    "\n",
    "# Add legend\n",
    "ax.legend()\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33032450-11c4-4d8a-a577-54e6c88ba3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_sim_inputs_training = torch.tensor( norm_sim_training_set_df[['z_target_um', 'TOD_fs3','GVD']].values, dtype=torch.float)\n",
    "norm_sim_outputs_training = torch.tensor( norm_sim_training_set_df['n_protons'].values.reshape(-1,1), dtype=torch.float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937f3f90-df13-449f-b7e4-d0bdf136335c",
   "metadata": {},
   "source": [
    "<h2> Train base neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19fb2d7-57bd-4565-a401-93ab5996cb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_net = NN()\n",
    "sim_net.train_model(norm_sim_inputs_training, norm_sim_outputs_training,num_epochs=10000)\n",
    "sim_net.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6ba9f6-ef5f-415d-9f3f-d10dedd30833",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_train_predictions = sim_net.predict(norm_sim_inputs_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a956c7f7-85b0-486f-880e-2c8354481e96",
   "metadata": {},
   "source": [
    "<h2> Visualize NN prediction on same data that it was training on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde8f271-7289-4f83-9f8f-814a660094b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(norm_sim_training_set_df['z_target_um'], norm_sim_training_set_df['n_protons'], label='Simulation training set')\n",
    "\n",
    "ax.scatter(norm_sim_training_set_df['z_target_um'], sim_train_predictions, label='predictions', s=50, facecolors='none', edgecolors='r')\n",
    "\n",
    "plt.title(\"n_protons predictions\")\n",
    "plt.xlabel('z_target (m)')\n",
    "plt.ylabel('Number of protons (1/sr)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dfaf2f-b2e5-498b-a639-3de8ed4108dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Scatter plot for training set\n",
    "ax.scatter(norm_sim_training_set_df['z_target_um'], norm_sim_training_set_df['TOD_fs3'], norm_sim_training_set_df['n_protons'], label='Training Set 1', alpha=0.7)\n",
    "\n",
    "\n",
    "ax.scatter(norm_sim_training_set_df['z_target_um'], norm_sim_training_set_df['TOD_fs3'], sim_train_predictions, label='predictions 1', s=50, facecolors='none', edgecolors='r')\n",
    "#ax.scatter(test_predictions['TOD'], test_predictions['Z_target'], test_predictions['predictions'], s=50, facecolors='none', edgecolors='r')\n",
    "\n",
    "ax.view_init(elev=40., azim=40)\n",
    "# Set labels and title\n",
    "ax.set_title('Simulation Data v Predictions')\n",
    "ax.set_xlabel('TOD')\n",
    "ax.set_ylabel('z_target')\n",
    "ax.set_zlabel('n Protons')\n",
    "\n",
    "# Add legend\n",
    "ax.legend()\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6fbec8-4640-4119-ae82-ac54b2c5330f",
   "metadata": {},
   "source": [
    "<h2> Define NN that adds linear calibration parameters to output obtained from pre-trained base neural net </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfdcefa-fcac-4ada-8023-1f2019e64f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class finetune_NN(NN):\n",
    "    def __init__(self, base_model, hidden_size=20, \n",
    "                 learning_rate=0.001, patience=100, factor=0.5, threshold=1e-4):\n",
    "        super(finetune_NN, self).__init__(hidden_size, learning_rate, patience, factor, threshold)\n",
    "        \n",
    "        # Copying weights from base_model to this model\n",
    "        self.hidden1.load_state_dict(base_model.hidden1.state_dict())\n",
    "        self.hidden2.load_state_dict(base_model.hidden2.state_dict())\n",
    "        self.hidden3.load_state_dict(base_model.hidden3.state_dict())\n",
    "        self.hidden4.load_state_dict(base_model.hidden4.state_dict())\n",
    "        self.hidden5.load_state_dict(base_model.hidden5.state_dict())        \n",
    "        self.output.load_state_dict(base_model.output.state_dict())\n",
    "        # This is the weight and bias to be tuned on the output obtained from base model\n",
    "        self.w2 =torch.nn.Parameter(torch.rand(1))\n",
    "        self.b2 = torch.nn.Parameter(torch.rand(1))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, 'min', \n",
    "                                           factor=factor, patience=patience, threshold=threshold)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.relu(self.hidden2(x))\n",
    "        x = self.relu(self.hidden3(x))\n",
    "        x = self.relu(self.hidden4(x))\n",
    "        x = self.relu(self.hidden5(x))\n",
    "        x = self.output(x)\n",
    "        x = x * self.w2 + self.b2\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "    def train_model(self, inputs, outputs, num_epochs=1500):\n",
    "        '''\n",
    "        args:\n",
    "            tensor inputs input dataset to train NN\n",
    "            tensor outputs: output dataset to train NN\n",
    "            int num_epochs: iterations of training\n",
    "        '''\n",
    "        oputputs = outputs.to(torch.float32)\n",
    "        inputs = inputs.to(torch.float32)\n",
    "        for param in self.hidden1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.hidden2.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.hidden3.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.hidden4.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.hidden5.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.output.parameters():\n",
    "            param.requires_grad = False   \n",
    "        self.train()\n",
    "\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            predictions = self(inputs)\n",
    "            loss = self.criterion(predictions, outputs)\n",
    "            loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            current_loss = loss.item()\n",
    "            self.loss_data['loss'].append(loss.detach().numpy())\n",
    "            self.loss_data['epoch_count'].append(epoch)\n",
    "            self.scheduler.step(current_loss)\n",
    "\n",
    "            if(epoch+1) % (num_epochs/10) == 0:\n",
    "                print(f'Comb NN: Epoch [{epoch+1}/{num_epochs}], Loss:{loss.item():.6f}')\n",
    "                print(\"w2,b2\",self.w2,self.b2)\n",
    "\n",
    "    def test_model(self, inputs, outputs):\n",
    "        '''\n",
    "        args:\n",
    "            tensor inputs: an input dataset to pass through NN and test\n",
    "            tensor outputs: an output dataset to pass through NN\n",
    "        '''\n",
    "        inputs = inputs.to(torch.float32)\n",
    "        outputs = outputs.to(torch.float32)\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = self(inputs)\n",
    "            loss = self.criterion(predictions, outputs).item()\n",
    "\n",
    "            print(f'Test Loss: {loss:.4f}')\n",
    "\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        '''\n",
    "        args:\n",
    "            tensor inputs\n",
    "        returns:\n",
    "            numpy array with predictions\n",
    "        '''\n",
    "        inputs = inputs.to(torch.float32)\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self(inputs)\n",
    "            predictions = output.detach().numpy()\n",
    "\n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdb59b0-4ea5-46cc-afa3-892fc4a852e4",
   "metadata": {},
   "source": [
    "<h2> Normalize experimental data with AffineInput </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d89dec-fd0b-4eea-9ee2-f8df9d41e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply normalization to the training and test sets from experiments\n",
    "norm_expt_training_set_df = expt_training_set_df.copy()\n",
    "norm_expt_training_set_df[['z_target_um', 'TOD_fs3','GVD']] = input_transform( torch.tensor( expt_training_set_df[['z_target_um', 'TOD_fs3','GVD']].values ) )\n",
    "norm_expt_training_set_df[['n_protons']] = output_transform( torch.tensor( expt_training_set_df[['n_protons']].values.reshape(-1,1) ) )\n",
    "\n",
    "\n",
    "norm_expt_test_set_df = expt_test_set_df.copy()\n",
    "norm_expt_test_set_df[['z_target_um', 'TOD_fs3','GVD']] = input_transform( torch.tensor( expt_test_set_df[['z_target_um', 'TOD_fs3','GVD']].values ) )\n",
    "norm_expt_test_set_df[['n_protons']] = output_transform( torch.tensor( expt_test_set_df[['n_protons']].values.reshape(-1,1) ) )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb614072-bfc1-45c7-b7f0-3325ef70da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(norm_sim_training_set_df['z_target_um'], norm_sim_training_set_df['n_protons'], label='Simulation training set')\n",
    "ax.scatter(norm_expt_training_set_df['z_target_um'], norm_expt_training_set_df['n_protons'], label='Expt training set')\n",
    "ax.scatter(norm_expt_test_set_df['z_target_um'], norm_expt_test_set_df['n_protons'], label='Expt test set')\n",
    "\n",
    "\n",
    "plt.title(\"n_protons predictions\")\n",
    "plt.xlabel('z_target (m)')\n",
    "plt.ylabel('Number of protons (1/sr)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471a190e-ffbe-4b2c-bc8e-bffeed455684",
   "metadata": {},
   "source": [
    "<h2> Train the linear scaling parameters on the output, namely, w2, b2, on the training set of experimental data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593a406c-bb9b-4a5e-9465-07580bdbab4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_expt_inputs_training = torch.tensor( norm_expt_training_set_df[['z_target_um', 'TOD_fs3','GVD']].values, dtype=torch.float)\n",
    "norm_expt_outputs_training = torch.tensor( norm_expt_training_set_df['n_protons'].values.reshape(-1,1), dtype=torch.float)\n",
    "\n",
    "norm_expt_inputs_test = torch.tensor( norm_expt_test_set_df[['z_target_um', 'TOD_fs3','GVD']].values, dtype=torch.float)\n",
    "norm_expt_outputs_test = torch.tensor( norm_expt_test_set_df['n_protons'].values.reshape(-1,1), dtype=torch.float)\n",
    "\n",
    "\n",
    "finetune_net = finetune_NN(sim_net,learning_rate=0.01)\n",
    "finetune_net.train_model(norm_expt_inputs_training,norm_expt_outputs_training,num_epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ffb5b-66a0-4079-8c0c-b5d17a78eab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the loss and print the values of the learned calibration parameters \n",
    "finetune_net.plot_loss()\n",
    "for name, param in finetune_net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a46eb2-06c4-4946-8a7d-e2815ea17bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions from calibrated NN on testing data from experiments\n",
    "finetune_net_pred_expt_test = finetune_net.predict(norm_expt_inputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b105be29-589a-4cfd-9b06-72ff5a14e61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions from calibrated NN on testing data from experiments\n",
    "finetune_net_pred_expt_train = finetune_net.predict(norm_expt_inputs_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543734d4-65ec-4d6e-ae35-f65ef81b0f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "simnet_expt_train_pred = sim_net.predict(norm_expt_inputs_training)\n",
    "simnet_expt_test_pred = sim_net.predict(norm_expt_inputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c2f962-3592-48dc-9bd5-3c9acc71a6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "ax.scatter(norm_sim_training_set_df['z_target_um'], norm_sim_training_set_df['n_protons'], label='Simulation training set')\n",
    "ax.scatter(norm_expt_training_set_df['z_target_um'], norm_expt_training_set_df['n_protons'], label='Expt training set')\n",
    "ax.scatter(norm_expt_test_set_df['z_target_um'], norm_expt_test_set_df['n_protons'], label='Expt test set')\n",
    "\n",
    "ax.scatter(norm_expt_training_set_df['z_target_um'], finetune_net_pred_expt_train, label=' expt train predictions', s=50, facecolors='none', edgecolors='m')\n",
    "ax.scatter(norm_expt_test_set_df['z_target_um'], finetune_net_pred_expt_test,label=' expt test predictions', s=50, facecolors='none', edgecolors='r')\n",
    "\n",
    "# ax.scatter(norm_expt_training_set_df['z_target_um'], simnet_expt_train_pred, label='exp train predictions from uncalibrated base NN', s=50, facecolors='none', edgecolors='cyan')\n",
    "# ax.scatter(norm_expt_test_set_df['z_target_um'], simnet_expt_test_pred,label=' expt test predictions from uncalibrated base NN', s=50, facecolors='none', edgecolors='k')\n",
    "\n",
    "\n",
    "plt.title(\"n_protons predictions\")\n",
    "plt.xlabel('z_target (m)')\n",
    "plt.ylabel('Number of protons (1/sr)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6039cdd-52c7-4f80-8f5e-ed367b3b2513",
   "metadata": {},
   "source": [
    "<h2> Saving the Lume Model </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44729001-6a93-46fa-bf94-2688f1ab19cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lume_model.models import TorchModel\n",
    "from lume_model.variables import ScalarInputVariable, ScalarOutputVariable\n",
    "model = TorchModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658a1dfb-35d6-4372-a88e-e3f00fcd9001",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_variables = [\n",
    "    ScalarInputVariable(name=\"z_target_um\", default=0, value_range=[-150,150]),\n",
    "    ScalarInputVariable(name=\"TOD_fs3\", default=0, value_range=[-8e4,8e4]),\n",
    "    ScalarInputVariable(name=\"GVD\", default=0, value_range=[13.2,13.9])\n",
    "]\n",
    "output_variables = [\n",
    "    ScalarOutputVariable(name=\"n_protons\", default=0, value_range=[0,8e10])\n",
    "]\n",
    "\n",
    "#Storing the weights and biases that calibrate the output\n",
    "output_weight_params = []\n",
    "output_bias_params = []\n",
    "\n",
    "# Iterate over the model's parameters\n",
    "for name, param in finetune_net.named_parameters():\n",
    "    # Check if the parameter requires gradients and its name contains 'w'\n",
    "    if param.requires_grad:\n",
    "        print(name,param)\n",
    "    if param.requires_grad and 'w' in name.lower():\n",
    "        # Append the parameter's name to the array\n",
    "        output_weight_params.append(param.data)\n",
    "    if param.requires_grad and 'b' in name.lower():\n",
    "        # Append the parameter's name to the array\n",
    "        output_bias_params.append(param.data)\n",
    "\n",
    "\n",
    "##### Here as well -- assuming that we know that param_stored[0] is w, and param_stored[1] is bias\n",
    "calibration_transform = AffineInputTransform( 1, coefficient=torch.tensor(output_weight_params), offset=torch.tensor(output_bias_params))\n",
    "\n",
    "model = TorchModel(\n",
    "    model=sim_net,\n",
    "    input_variables=input_variables,\n",
    "    output_variables=output_variables,\n",
    "    input_transformers=[input_transform],\n",
    "    output_transformers=[calibration_transform,output_transform] # saving calibration before normalization\n",
    ")\n",
    "\n",
    "model.dump( file='base_simulation_model_with_transformers.yml' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3ac17b-167e-43b3-8675-101b9c212ffd",
   "metadata": {},
   "source": [
    "<h2> LoadingModel </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8485456a-c039-4869-9a4c-ad702b3d7199",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = TorchModel('base_simulation_model_with_transformers.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100dcf60-6bb1-4572-b2c3-31b61c92ef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "\n",
    "\n",
    "expt_training_set_df = pd.read_csv('../../experimental_data/training_set_1.csv')\n",
    "expt_test_set_df = pd.read_csv('../../experimental_data/test_set_1.csv')\n",
    "\n",
    "\n",
    "ax.scatter( expt_training_set_df['TOD_fs3'], expt_training_set_df['z_target_um'],\n",
    "            expt_training_set_df['n_protons'], alpha=0.7, label='Expt Training Set')\n",
    "ax.scatter( expt_test_set_df['TOD_fs3'], expt_test_set_df['z_target_um'],\n",
    "            expt_test_set_df['n_protons'], alpha=0.7, label='Expt Training Set')\n",
    "\n",
    "n_predict = loaded_model.evaluate(\n",
    "    {label: torch.tensor( expt_training_set_df[label].values ) for label in ['z_target_um', 'TOD_fs3', 'GVD']})\n",
    "ax.scatter( expt_training_set_df['TOD_fs3'], expt_training_set_df['z_target_um'],\n",
    "           n_predict['n_protons'], s=50, facecolors='none', edgecolors='r', label='prediction on training set')\n",
    "\n",
    "\n",
    "n_predict = loaded_model.evaluate(\n",
    "    {label: torch.tensor( expt_test_set_df[label].values ) for label in ['z_target_um', 'TOD_fs3', 'GVD']})\n",
    "ax.scatter( expt_test_set_df['TOD_fs3'], expt_test_set_df['z_target_um'],\n",
    "           n_predict['n_protons'], s=50, facecolors='none', edgecolors='r', label='prediction on test set')\n",
    "\n",
    "ax.view_init(elev=40., azim=40, roll=0)\n",
    "plt.xlabel('TOD')\n",
    "plt.ylabel('z_target')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c97c07-ded2-44e5-b40d-738d2eb01f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "ax = plt.figure().add_subplot()\n",
    "\n",
    "\n",
    "expt_training_set_df = pd.read_csv('../../experimental_data/training_set_1.csv')\n",
    "expt_test_set_df = pd.read_csv('../../experimental_data/test_set_1.csv')\n",
    "\n",
    "ax.scatter( expt_training_set_df['z_target_um'],\n",
    "            expt_training_set_df['n_protons'], alpha=0.7, label='Expt train set')\n",
    "ax.scatter( expt_test_set_df['z_target_um'],\n",
    "           expt_test_set_df['n_protons'], alpha=0.7, label='Expt test set')\n",
    "\n",
    "\n",
    "n_predict = loaded_model.evaluate(\n",
    "    {label: torch.tensor( expt_training_set_df[label].values ) for label in ['z_target_um', 'TOD_fs3', 'GVD']})\n",
    "ax.scatter( expt_training_set_df['z_target_um'],\n",
    "           n_predict['n_protons'], s=50, facecolors='none', edgecolors='r', label='Prediction on expt train set')\n",
    "\n",
    "\n",
    "n_predict = loaded_model.evaluate(\n",
    "    {label: torch.tensor( expt_test_set_df[label].values ) for label in ['z_target_um', 'TOD_fs3', 'GVD']})\n",
    "ax.scatter( expt_test_set_df['z_target_um'],\n",
    "           n_predict['n_protons'], s=50, facecolors='none', edgecolors='b', label='Prediction on expt test set')\n",
    "\n",
    "\n",
    "plt.xlabel('TOD')\n",
    "plt.ylabel('z_target')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
