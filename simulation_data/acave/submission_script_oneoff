#!/bin/bash -l

#SBATCH -t 00:30:00
#SBATCH -N 3
#SBATCH -A m558_g
#SBATCH -C gpu
#SBATCH -q debug
#SBATCH --exclusive
#SBATCH --gpu-bind=none
#SBATCH --ntasks-per-node=4
#SBATCH --gpus-per-node=4

# Source environment
source $HOME/db.profile
source $HOME/perlmutter_gpu_warpx.profile

# TODO: adapt directory name
cd $SCRATCH/2024_IFE-superfacility/simulation_data/acave
mkdir oneoff_$SLURM_JOB_ID
cd oneoff_$SLURM_JOB_ID

# Execute python script to prepare lasy file
../templates/prepare_simulation.py

# Launch warpx simulation
export SRUN_CPUS_PER_TASK=16
GPU_AWARE_MPI="amrex.use_gpu_aware_mpi=1"
srun --cpu-bind=cores bash -c "
    export CUDA_VISIBLE_DEVICES=\$((3-SLURM_LOCALID));
    ../templates/warpx.rz ../templates/template_inputs ${GPU_AWARE_MPI} warpx.numprocs=1 12 > output.txt"

# Execute analysis script
../templates/analyze_simulation.py
